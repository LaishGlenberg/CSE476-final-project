{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf')):\n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "3\n",
      "[{'char_count': 274,\n",
      "  'exp_word_count': 59,\n",
      "  'expected': '\\n'\n",
      "              '    exit_codes = []\\n'\n",
      "              '\\n'\n",
      "              '    def execute_file(file):\\n'\n",
      "              '        file_path = file\\n'\n",
      "              '        process = subprocess.Popen(file_path)\\n'\n",
      "              '        time.sleep(1)  # wait for the process to start\\n'\n",
      "              '        exit_codes.append(process.poll())  # store the exit '\n",
      "              'code\\n'\n",
      "              '\\n'\n",
      "              '    # Start a thread for each file\\n'\n",
      "              '    threads = [threading.Thread(target=execute_file, '\n",
      "              'args=(file,)) for file in file_list]\\n'\n",
      "              '    for thread in threads:\\n'\n",
      "              '        thread.start()\\n'\n",
      "              '\\n'\n",
      "              '    # Wait for all threads to finish\\n'\n",
      "              '    for thread in threads:\\n'\n",
      "              '        thread.join()\\n'\n",
      "              '\\n'\n",
      "              '    return exit_codes',\n",
      "  'id': 'coding_0_99_99',\n",
      "  'prompt': 'Run files from list of files as subprocesses at the same time.\\n'\n",
      "            'The function should output with:\\n'\n",
      "            '    list: The exit codes of the subprocesses.\\n'\n",
      "            'You should write self-contained code starting with:\\n'\n",
      "            '```\\n'\n",
      "            'import subprocess\\n'\n",
      "            'import time\\n'\n",
      "            'import threading\\n'\n",
      "            'def task_func(file_list):\\n'\n",
      "            '```',\n",
      "  'type': 'coding'},\n",
      " {'char_count': 65,\n",
      "  'exp_word_count': 1,\n",
      "  'expected': 'Canada',\n",
      "  'id': 'common_sense_1_294_394',\n",
      "  'prompt': 'Iqaluit Airport and Canadian North are based out of what country?',\n",
      "  'type': 'common_sense'},\n",
      " {'char_count': 161,\n",
      "  'exp_word_count': 4,\n",
      "  'expected': '\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)',\n",
      "  'id': 'math_3_210_810',\n",
      "  'prompt': 'Convert the point $(0,3)$ in rectangular coordinates to polar '\n",
      "            'coordinates.  Enter your answer in the form $(r,\\\\theta),$ where '\n",
      "            '$r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
      "  'type': 'math'}]\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=-1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output):\n",
    "    out = output.lower().strip('.')\n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    \n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got)\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        seperator('\\nEVALUATION')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        \n",
    "        if not agreement:\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            \n",
    "            if not not_truncated and correctness:\n",
    "                correctness = False\n",
    "                print(\"‚ùå INCORRECT | BOTH EVALS RETURNED TRUE BUT OUTPUT WAS TRUNCATED\")\n",
    "\n",
    "        final_answers.append(correctness)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return final_answers\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 1000\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"common_sense_1_269_369\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Is a Halloween cruise in the Gulf of Mexico likely to be safe from storms?\",\n",
      "  \"expected\": false,\n",
      "  \"char_count\": 74,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 3/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "false"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: True\n",
      "self_eval: True\n",
      "‚úÖ CORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"math_3_289_889\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Simplify: $\\\\frac{\\\\sqrt{2.5^2-0.7^2}}{2.7-2.5}$.\",\n",
      "  \"expected\": \"12\",\n",
      "  \"char_count\": 47,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 6/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "$1.2$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"common_sense_1_65_165\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Who had a 70s No 1 hit with Billy, Don't Be A Hero? Answer the question using the context.\\n\\n \\\"Billy Don't Be a Hero\\\" is a 1974 pop song that was first a hit in the UK for Paper Lace and then some months later it was a hit in the US for Bo Donaldson and The Heywoods. The song was written by two British songwriters Mitch Murray and Peter Callander.\\n\\nBecause the song was released in 1974, it was associated by some listeners with the Vietnam War, though it actually refers to an unidentified war.  But the drum pattern, references to a marching band leading soldiers in blue, and \\\"riding out\\\" (cavalry) would seem to be referencing the American Civil War.\\n\\nA young woman is distraught that her fianc√© chooses to leave the area with Army recruiters passing through the town and go with them to fight. She laments,\\n\\nThe song goes on to describe how Billy is killed in action in a pitched battle after volunteering to ride out and seek reinforcements (which suggests mounted infantry and a lack of modern two-way radio communications). In the end, the woman throws away the official letter notifying her of Billy's \\\"heroic\\\" death.\\n\\nChart performances\\n\\nPaper Lace's version of \\\"Billy Don't Be a Hero\\\" hit number one in the UK Singles Chart on 16 March 1974,  and thereafter Bo Donaldson & The Heywoods version hit number one in the U.S. on the Billboard Hot 100 on 15 June 1974, and number one in Canada on 7 July. The US version sold over three and a half million copies, and was awarded a gold disc by the R.I.A.A. in June 1974.  The Bo Donaldson version was a massive hit in North America but is largely unknown elsewhere. Billboard ranked it as the No. 21 song for 1974. \\n\\nQuoted in other media\\n\\nThe song is mentioned as having played on K-Billy's Super Sounds of the 70s Weekend in the film Reservoir Dogs.\\n\\nThe song features in the film The Adventures of Priscilla, Queen of the Desert (1993).\\n\\nMystery Science Theater 3000 often riffs on movies by saying \\\"Billy, don't be a hero!\\\", including the episode \\\"The Creeping Terror\\\".\\n\\nIn the first episode of Friends, Ross is sad because it has been so long since he last picked up a woman, saying \\\"Do the words 'Billy, Don't Be a Hero' mean anything to you?\\\"\\n\\nMassive Attack's 1991 track \\\"Blue Lines\\\" (from the album of the same name) features the lyrics \\\"take a walk, Billy, don't be a hero\\\".\\n\\nIn Walk Hard: The Dewey Cox Story, the song is briefly heard during a montage in a disco cover by Dewey Cox (John C. Reilly) performing on rollerblades during \\\"The Dewey Cox Show\\\". A much longer cut of this scene can be seen in the director's cut, and the whole performance was included in the extras for the 2-Disc editions.\\n\\nIn the Powerpuff Girls, the leader of the Gang Green Gang, Ace, says to another member, Billy, \\\"Billy, don't be a hero!\\\" when he decides to save the Powerpuff Girls from a subway train.\\n\\nIn The Marvelous Misadventures of Flapjack, the episode \\\"K'nuckles, Don't Be a Hero\\\" is named after the song.\\n\\nIn The Justice Friends (Cartoon Network, 1996), Major Glory says \\\"Billy, don't be a hero!\\\" to William, Valhallen's pet goat, when it jumps to save Krunk from the attack of Valhallen's living clothes.\\n\\nIn an episode of ALF, Alf uses the line \\\"Willie, don't be a hero, don't be a fool with your life,\\\" referring to the head of the household, Wille Tanner, after Willie comes up with a bad idea.\\n\\nThe Doug Anthony Allstars performed a comedic cover of this song, featuring the altered line, \\\"Where did Billy's head go?\\\" in place of \\\"Keep your pretty head low\\\".\\n\\nDav Pilkey named the hero of The Adventures of Super Diaper Baby Billy solely to make possible a passing homage to Billy Don't Be a Hero.\\nPaper Lace are a Nottingham-based pop group who rose to sudden, brief success in 1974.  They are known to Americans as a one-hit wonder; however, they had other hits in the UK. Their best known songs are \\\"Billy Don't Be a Hero\\\" and \\\"The Night Chicago Died\\\".\\n\\nHistory\\n\\nThe core of the band originally formed in 1967 as Music Box, but changed their name to Paper Lace in 1969. Paper Lace was one of hundreds of pop bands in England looking for the big time while slogging their way through small club gigs and brief television appearances. A season at Tiffany's, a Rochdale club, and also at The Birdcage in Ashton-Under-Lyne in 1971, led to more television appearances, but a passport to the charts did not arrive until a 1973 victory on Opportunity Knocks, the ITV talent contest series.\\nAccording to drummer and lead singer Phil Wright:\\nOpportunity Knocks was pretty much the 1970s version of The X-Factor. There was a huge audition week in 1970 at the Bridgford Hotel, which is now the Rushcliffe Borough Council building near the City Ground. And there were thousands of people queuing up. We turned up in our best suits, did a few numbers, and were told that they liked us but not to expect to go on straight away. When they finally got back to us in 1973, we thought; do we really need this now? But they were getting viewing figures of 7 million, so we went for it. And we won five weeks on the trot! There were two songwriters (Mitch Murray and Peter Callander) who got in touch with our management and offered us \\\"Billy Don't Be a Hero\\\", with the possibility of more songs if it took off. We went down, recorded it, and they said \\\"Hey, this is a great song, it's going to be a hit\\\". And the song proved to be stronger than the band, because everyone knows it, they just can't remember who recorded it. Except in Nottingham, of course‚Ä¶Needham, Al. [http://www.leftlion.co.uk/articles.cfm/id/621 Paper Lace interview]. LeftLion.co.uk. Retrieved 8 September 2006.\\n\\nThanks to that show, songwriters/producers, Mitch Murray and Peter Callender quickly signed them. The smash hit \\\"Billy Don't Be a Hero\\\" spent three weeks at Number 1 on the UK Singles Chart in March 1974. It was followed by the story song \\\"The Night Chicago Died\\\" which reached Number 3. Another release, \\\"The Black-Eyed Boys\\\", took Paper Lace to number 11 in late 1974[http://www.sonsandlovers.co.uk/PAPER%20LACE.htm \\\"Paper Lace\\\"]. [http://www.sonsandlovers.co.uk/ Sons and Lovers website], 2003. Retrieved 9 September 2006. and number 37 in Canada.\\n\\nWith their subject matter assumed in America to be about the American Civil War, it was logical that \\\"Billy Don't Be a Hero\\\" should become a hit in the United States; however, Bo Donaldson and the Heywoods were the first to release \\\"Billy\\\" in the United States, and Paper Lace had to be content with a #96 placing. However, the follow-up song \\\"The Night Chicago Died\\\", set in the Prohibition era with reference to Al Capone, was untroubled by any such competition and topped the Billboard Hot 100. It sold over three million copies, and was awarded a gold disc by the R.I.A.A. in August 1974. \\n\\nAccording to Phil Wright:\\nWell, that [Chicago song] was even more successful. Number 1 in America. I got a platinum disc for that... and I certainly didn't give that away! I remember us being on Top Of The Pops and Elton John shaking my hand backstage and congratulating us on a US No.1, which he hadn't achieved at the time! The really strange thing was we couldn't even perform the song in America, due to some contractual hassles. And the label told us that they could make it a hit without us having to be there. We did a few radio stations, but that's all.\\n\\nThe group released two albums, First Edition (1972) and Paper Lace and Other Bits of Material (1974) however, they quickly faded from the public eye as the band's popularity waned. Philip Wright and Cliff Fish carried on as Paper Lace, with other musicians filling in for the missing band members. In 1978, they surfaced briefly with a sing-along version of \\\"We've Got the Whole World in Our Hands\\\" with their local football team, Nottingham Forest F.C. (Sendra, 2006). The 7\\\" single, with \\\"The Nottingham Forest March\\\" as the B-side, reached Number 24 in the UK chart but went Top 10 in the Netherlands.\\n\\nIn 1997, Wright joined Sons and Lovers, but he has occasional gigs billed as Philip Wright's Paper Lace.\\n\\nThey were the most successful band Nottingham ever produced, and were invited to perform on the Royal Variety Performance in front of the Queen Mother.\\n\\nOriginal Hit band members\\n\\n*\\n* Philip Wright (born 9 April 1948, St. Ann's, Nottingham, England) ‚Äî Drums / Lead Vocals\\n*Mick Vaughan (born Michael Vaughan, 27 July 1950, Sheffield, Yorkshire) - Lead/Rhythm Guitar/Arranger\\n*Cliff Fish (born Clifford Fish, 13 August 1949, Ripley) - Bass Guitar\\n*Chris Morris (born Christoper Morris, 1 November 1954, Nottingham, England) - Guitar -  Vocals\\n* Carlo Paul Santanna [born 1947] guitar/lead vocals.\\n\\nLater band members\\n\\n* Chris Raynor - joined 1978 [formally Billy Fury's guitarist]\\n* Jamie Moses\\n* Peter  Oliver, formerly  of  the  New  Seekers  1975-78\\n\\nDiscography\\n\\nAlbums\\n\\n*First Edition (March 1972) PHILIPS 6382 101 / reissued on CONTOUR 6870 637\\n\\nSide 1\\n# \\\"In the Morning\\\"\\n# \\\"Stoney End\\\"\\n# \\\"Lady\\\"\\n# \\\"I've Got You That's Enough For Me\\\"\\n# \\\"Threw My Love Away\\\"\\n# \\\"Martha (Whatever Happened)\\\"\\n\\nSide 2\\n# \\\"Games People Play\\\"\\n# \\\"Please Be My Friend\\\"\\n# \\\"You Can't Touch Me\\\"\\n# \\\"Elsie\\\"\\n# \\\"Like a Rolling Stone\\\"\\n# \\\"Early One Morning\\\"\\n\\n*And Other Bits of Material (June 1974) BUS STOP BUSLP 8001\\n\\nSide 1\\n#\\\"Billy Don't Be a Hero\\\"\\n#\\\"Hitchin' a Ride '74\\\"\\n#\\\"I Did What I Did for Maria\\\"\\n#\\\"Mary in the Morning\\\"\\n#\\\"Sealed with a Kiss\\\"\\n#\\\"Bye Bye Blues\\\"\\n\\nSide 2\\n#\\\"Happy Birthday Sweet Sixteen\\\"\\n#\\\"The Night Chicago Died\\\"\\n#\\\"Love Song\\\"\\n#\\\"Dreams Are Ten a Penny\\\"\\n#\\\"Love You're a Long Time Coming\\\"\\n#\\\"Cheek to Cheek\\\"\\n\\n*Paper Lace (1974, USA) Mercury SRM-1-1008\\n\\nSide 1\\n#\\\"The Night Chicago Died\\\" ‚Äì 3:30\\n#\\\"Billy Don't Be a Hero\\\" ‚Äì 3:55\\n#\\\"Hitchin' a Ride '74\\\" ‚Äì 2:45\\n#\\\"Sealed with a Kiss\\\" ‚Äì 3:01\\n#\\\"Love Song\\\" ‚Äì 4:10\\n#\\\"Love ‚Äì You're a Long Time Coming\\\" ‚Äì 2:50\\n\\nSide 2\\n#\\\"The Black‚ÄìEyed Boys\\\" ‚Äì 3:45\\n#\\\"Dreams Are Ten a Penny\\\" ‚Äì 2:30\\n#\\\"Mary in the Morning\\\" ‚Äì 3:04\\n#\\\"I Did What I Did for Maria\\\" ‚Äì 3:49\\n#\\\"Happy Birthday Sweet Sixteen\\\" ‚Äì 3:00\\n#\\\"Cheek to Cheek\\\" ‚Äì 3:23\\n\\nSingles\\n\\n*\\\"You Can't Touch Me\\\" / \\\"I've Got You, That's Enough for Me\\\" (1971) Concord CON 020\\n*\\\"In the Morning (Morning of My Life)\\\" / \\\"Elsie\\\" (14 January 1972) Concord CON 021\\n*\\\"Raggamuffin Man\\\" / \\\"Martha (Whatever Happened)\\\" (18 May 1973) Concord CON 027\\n*\\\"Billy Don't Be A Hero\\\" / \\\"Celia\\\" (11 January 1974)  Bus Stop Bus 1014 (# 96 Hot 100 / US)\\n*\\\"The Night Chicago Died\\\" / \\\"Can You Get It When You Want It\\\" (3 May 1974) Bus Stop Bus 1016 (# 1 for 1 week, Hot 100 / US)\\n*\\\"The Black Eyed Boys\\\" / \\\"Jean\\\" (July 1974) Bus Stop Bus 1019 (# 41 Hot 100 / US)\\n*\\\"Hitchin' A Ride '75\\\" / \\\"Love You're a Long Time Coming\\\" (7 February 1975) Bus Stop Bus 1024\\n*\\\"So What If I Am\\\" / \\\"Himalayan Lullaby\\\" (6 June 1975) Bus Stop Bus 1026\\n*\\\"I Think I'm Gonna Like It\\\" / \\\"Lost Love\\\" (23 July 1976) EMI EMI 2486\\n*\\\"We've Got The Whole World In Our Hands\\\" / \\\"The Forrest March\\\" (24 February 1978) Warner Bros K 171i7\\n\\nCD releases\\n\\n*Paper Lace and Other Material / First Edition (Double CD) - Cherry Red / 7t's Label  Cat No. Glam Cdd 109; both albums, plus B-sides to all singles released until 1975\",\n",
      "  \"expected\": \"bo donaldson heywoods\",\n",
      "  \"char_count\": 11068,\n",
      "  \"exp_word_count\": 3\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 3/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Paper Lace"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"math_3_183_783\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Carolyn practices the piano for 20 minutes a day and the violin for three times as long. If she practice six days a week, how many minutes does she spend practicing in a month with four weeks?\",\n",
      "  \"expected\": \"First find Carolyn's total violin practice time by tripling her piano practice time: 20 minutes/day * 3 = <<20*3=60>>60 minutes/day\\nThen find the total amount of time she spends practicing each day: 60 minutes/day + 20 minutes/day = <<60+20=80>>80 minutes/day\\nThen find the total time she spends practicing each week: 80 minutes/day * 6 days/week = <<80*6=480>>480 minutes/week\\nThen find the total time she spends practicing each month: 480 minutes/week * 4 weeks/month = <<480*4=1920>>1920 minutes/month\\n#### 1920\",\n",
      "  \"char_count\": 192,\n",
      "  \"exp_word_count\": 78\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 90/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "First, calculate the time spent on violin: 20 minutes √ó 3 = 60 minutes.  \n",
       "Then, total practice time per day: 20 + 60 = 80 minutes.  \n",
       "Weekly practice time: 80 minutes √ó 6 = 480 minutes.  \n",
       "Monthly practice time: 480 minutes √ó 4 = 1920 minutes.  \n",
       "\n",
       "1920"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: True\n",
      "‚ùå INCORRECT\n",
      "üÜò DISAGREED\n",
      "\n",
      "DISAGREEMENT --> SECOND EVAL\n",
      "--------------------------------\n",
      "self_eval2: True\n",
      "‚úÖ CORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"common_sense_1_323_423\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Will twenty pea pods contents cover entire chess board?\",\n",
      "  \"expected\": true,\n",
      "  \"char_count\": 55,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 3/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "false"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "[True, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "test_prompts = get_tests(n=5) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "print(results_llm_judge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
