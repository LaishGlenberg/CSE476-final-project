{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393516c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "import requests\n",
    "\n",
    "API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")              \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"You are a helpful assistant. Reply with only the final answer‚Äîno explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.3,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        #{'id': 'chatcmpl-88b6d7e18a5542b5bed5bf2828f0661e', 'object': 'chat.completion', 'created': 1763204718, 'model': 'bens_model', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'US Highway 281', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 50, 'total_tokens': 57, 'completion_tokens': 7, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            #print(data)\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            tokens_used = data.get(\"usage\",[{}]).get(\"completion_tokens\", {})\n",
    "            #print('used tokens:', tokens_used)\n",
    "            \n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs, \"tokens_used\":tokens_used}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f36f76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Direct call example\n",
    "def direct_call(prompt=\"What is 17 + 28? Answer with just the number.\", temperature=0.2, max_tokens=128):\n",
    "    demo_prompt = prompt\n",
    "    result = call_model_chat_completions(demo_prompt, temperature=temperature, max_tokens=max_tokens)\n",
    "    print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "    print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "    # Optional: Inspect rate-limit headers if your provider exposes them\n",
    "    for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "        if k in result[\"headers\"]:\n",
    "            print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a3b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "my_tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9af2b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'common_sense': 400, 'math': 300, 'coding': 100, 'future_prediction': 100, 'planning': 100})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "POSSIBLE_TYPES = ['math', 'common_sense', 'planning', 'coding', 'future_prediction']\n",
    "\n",
    "all_tests = json.load(open(\"parsed_dev_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "type_counts = Counter(t['domain'] for t in all_tests)\n",
    "print(type_counts)\n",
    "\n",
    "formatted_tests = []\n",
    "for i, t in enumerate(all_tests, start=1):\n",
    "    \n",
    "    formatted_tests.append({\n",
    "        \"id\": t['id'], # domain_domainIndex_domainTestIndex_testIndex\n",
    "        \"type\": t['domain'],\n",
    "        \"prompt\": t['input'],\n",
    "        \"expected\": t['output'],\n",
    "        \"char_count\": t['input_char_count'],\n",
    "        \"exp_word_count\": t['exp_word_count']\n",
    "    })\n",
    "    \n",
    "all_tests = formatted_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe04856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\\n    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\\n    return tests[start:end]\\n\\ndef get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\\n    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\\n    sample_size = min(n, len(filtered_tests)) #prevent error\\n    return random.sample(filtered_tests, sample_size) \""
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_test(test):\n",
    "    print(json.dumps(test, indent=2, ensure_ascii=False))\n",
    "\n",
    "#pass test_type as a list of types\n",
    "#generalized get test function\n",
    "def get_tests(n=0, test_type=POSSIBLE_TYPES, start=0, end=None, lower_char=0, upper_char=float('inf'), lower_exp=0, upper_exp=float('inf')):\n",
    "    filtered_tests = [t for t in all_tests if t['type'] in test_type and lower_char <= t['char_count'] <= upper_char and lower_exp <= t['exp_word_count'] <= upper_exp]\n",
    "    print('filtered size:', len(filtered_tests))\n",
    "    sample_size = min(n, len(filtered_tests))\n",
    "    \n",
    "    if n == 0:\n",
    "        return filtered_tests[start:end]\n",
    "    elif n == -1:\n",
    "        filtered_type_counts = Counter(t['type'] for t in filtered_tests)\n",
    "        each_test = []\n",
    "        count = 0\n",
    "        \n",
    "        for val in filtered_type_counts.values():\n",
    "            rand = random.randint(count, count + val)\n",
    "            count = count + val\n",
    "            each_test.append(filtered_tests[rand])\n",
    "            \n",
    "        print(\"sampled size:\", len(each_test))    \n",
    "        return each_test\n",
    "    else:\n",
    "        return random.sample(filtered_tests, sample_size)\n",
    "    \n",
    "\"\"\" def get_test_type(test_type, start=0, end=None, lower=0, upper=float('inf')):\n",
    "    tests = [t for t in all_tests if t['type'] in test_type and lower <= t['char_count'] <= upper]\n",
    "    return tests[start:end]\n",
    "\n",
    "def get_random_tests(n=5, lower=0, upper=float('inf'), test_type=POSSIBLE_TYPES):\n",
    "    filtered_tests = get_test_type(test_type=test_type, lower=lower, upper=upper) #[t for t in all_tests if lower <= t['char_count'] <= upper]\n",
    "    sample_size = min(n, len(filtered_tests)) #prevent error\n",
    "    return random.sample(filtered_tests, sample_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 440\n",
      "['First',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'many',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'the',\n",
      " 'original',\n",
      " 'bolt',\n",
      " 'of',\n",
      " 'fabric',\n",
      " 'was:',\n",
      " '16',\n",
      " 'feet',\n",
      " '*',\n",
      " '12',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<16*12=192>>192',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'living',\n",
      " 'room',\n",
      " 'curtains:',\n",
      " '4',\n",
      " 'feet',\n",
      " '*',\n",
      " '6',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<4*6=24>>24',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Then',\n",
      " 'figure',\n",
      " 'out',\n",
      " 'how',\n",
      " 'much',\n",
      " 'fabric',\n",
      " 'Ann',\n",
      " 'took',\n",
      " 'for',\n",
      " 'the',\n",
      " 'bathroom',\n",
      " 'curtains:',\n",
      " '2',\n",
      " 'feet',\n",
      " '*',\n",
      " '4',\n",
      " 'feet',\n",
      " '=',\n",
      " '<<2*4=8>>8',\n",
      " 'square',\n",
      " 'feet',\n",
      " 'Finally,',\n",
      " 'subtract',\n",
      " 'the',\n",
      " 'square',\n",
      " 'footage',\n",
      " 'of',\n",
      " 'both',\n",
      " 'sets',\n",
      " 'of',\n",
      " 'curtains',\n",
      " 'from',\n",
      " 'the',\n",
      " 'total',\n",
      " 'square',\n",
      " 'footage:',\n",
      " '192',\n",
      " '-',\n",
      " '24',\n",
      " '-',\n",
      " '8',\n",
      " '=',\n",
      " '<<192-24-8=160>>160',\n",
      " 'square',\n",
      " 'feet',\n",
      " '####',\n",
      " '160']\n"
     ]
    }
   ],
   "source": [
    "tests = get_tests(n=1, upper_char=300) #get_test_type('math', end=10, lower=0, upper=500)\n",
    "pprint(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b72b0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple hello world call to kick off the commits\n",
    "#direct_call(prompt=\"how do I find the derivative of y=x^2 using python?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54e39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    messages = [\"<Start of message history>\"]\n",
    "    count = 0\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting chat.\")\n",
    "            break\n",
    "        response = call_model_chat_completions(prompt=f\"Old messages{messages}, CURRENT USER INPUT:{user_input} <--- ANSWER THIS QUESTION\", temperature=0.7)\n",
    "        count += 1\n",
    "        messages.append(f\"MESSAGE_{count}_[previous user input: {user_input}, previous system response: {response['text']}]\")\n",
    "        if response[\"ok\"]:\n",
    "            print(\"Model:\", response[\"text\"].strip())\n",
    "        else:\n",
    "            print(\"Error:\", response[\"error\"])\n",
    "        print(messages)\n",
    "#interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d8dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def execute_tests():\\n    rows = []\\n    for t in tests:\\n        r = call_model_chat_completions(\\n            prompt,\\n            system=system,\\n            model=model,\\n            temperature=0.3,\\n            max_tokens=128\\n        ) '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def execute_tests():\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            prompt,\n",
    "            system=system,\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=128\n",
    "        ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e38f632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ce4fb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate2(question, model_output, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly Yes or No. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"MODEL_1 thinks this ANSWER is {prediction}, do you agree with MODEL_1 decision?\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{model_output}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "-----------------------\n",
    "MODEL_1 OUTPUT:\n",
    "{prediction}\n",
    "-----------------------\n",
    "\n",
    "Answer with exactly: Yes or No. Do you agree with MODEL_1?\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\") or reply.startswith(\"yes\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\") or reply.startswith(\"no\"):\n",
    "        return False\n",
    "\n",
    "    # No Fallback yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e2ca1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_map = {'yes':'true', 'no':'false'}\n",
    "def map_tf(output):\n",
    "    out = output.lower().strip('.')\n",
    "    return tf_map.get(out) if out in tf_map else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_match_check(test, output):\n",
    "    exp = test[\"expected\"]\n",
    "    \n",
    "    output = map_tf(output)\n",
    "    \n",
    "    matches = re.findall(re.escape(str(exp)), output, re.IGNORECASE)\n",
    "    \n",
    "    num_matches = len(matches)\n",
    "    if num_matches > 0:\n",
    "        #print('MATCH(ES) FOUND:', matches)\n",
    "        return True\n",
    "    \n",
    "    #print('NO MATCH FOUND')\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "baa83c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperator(text, tokens_used=None, max_tokens=400):\n",
    "    if tokens_used is not None:\n",
    "        print(f'{text} (TOKENS USED: {tokens_used}/{max_tokens})')\n",
    "        if int(tokens_used) == max_tokens:\n",
    "            print('MAXED TOKENS REACHED - OUTPUT TRUNCATED')\n",
    "            return False\n",
    "    else:\n",
    "        print(text)\n",
    "    print('-'*32)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9228cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correct(bool1, bool2):\n",
    "    correctness = bool1 and bool2\n",
    "    agreement = bool1 == bool2\n",
    "    \n",
    "    print('‚úÖ CORRECT') if correctness else print('‚ùå INCORRECT')\n",
    "    print('üÜó AGREED') if agreement else print('üÜò DISAGREED')\n",
    "    \n",
    "    return correctness, agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c023deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_match(test, output):\n",
    "    expected = test[\"expected\"].lower().split()\n",
    "    output = output.lower().split()\n",
    "    \n",
    "    output_counter = Counter(output)\n",
    "    \n",
    "    match_counts = {expword: output_counter.get(expword, 0) for expword in expected}\n",
    "    total_matches = sum(match_counts.values())\n",
    "    output_len = len(output)\n",
    "    print(f\"{total_matches}/{output_len}\")\n",
    "    print('match counts:', match_counts)\n",
    "    \n",
    "    #return match_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7cdafb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    MAX_TOKENS = 400\n",
    "    final_answers = []\n",
    "    count = 0\n",
    "    \n",
    "    for t in tests:\n",
    "        count += 1\n",
    "        # 1) Get model prediction\n",
    "        #print('prompt:', t['prompt'])\n",
    "        print('\\n','='*64)\n",
    "        seperator('TEST_CASE')\n",
    "        print_test(t)\n",
    "        r = call_model_chat_completions(\n",
    "            f\"{t['prompt']}\",\n",
    "            system=\"Give a short answer to each prompt, don't explain.\",\n",
    "            model=model,\n",
    "            temperature=0.3,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "        tokens_used = r.get(\"tokens_used\")\n",
    "        \n",
    "\n",
    "        got = map_tf(got)\n",
    "        \n",
    "        #If output is truncated and both evals return true, return false\n",
    "        not_truncated = seperator('\\nMODEL_OUTPUT', tokens_used, MAX_TOKENS)\n",
    "        display(Markdown(f\"\\n{got}\"))\n",
    "        #print('raw: ', got)\n",
    "        \n",
    "        super_match(t, got)\n",
    "        match_check = basic_match_check(t, got)\n",
    "        match_check = bool(match_check)\n",
    "        \n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "        is_correct = bool(is_correct)\n",
    "        \n",
    "        seperator('\\nEVALUATION')\n",
    "        print('match check:', match_check)\n",
    "        print('self_eval:', is_correct)\n",
    "        correctness, agreement = check_correct(match_check, is_correct)\n",
    "        \n",
    "        if not agreement:\n",
    "            seperator('\\nDISAGREEMENT --> SECOND EVAL')\n",
    "            is_correct2 = self_evaluate2(\n",
    "                question=t[\"prompt\"],\n",
    "                model_output=got,\n",
    "                expected_answer=t[\"expected\"],\n",
    "                prediction=is_correct,\n",
    "                model=judge_model\n",
    "            )\n",
    "            is_correct2 = bool(is_correct2)\n",
    "            \n",
    "            print('self_eval2:', is_correct2)\n",
    "            correctness, agreement = check_correct(is_correct, is_correct2)\n",
    "            \n",
    "            if not not_truncated and correctness:\n",
    "                correctness = False\n",
    "                print(\"‚ùå INCORRECT | BOTH EVALS RETURNED TRUE BUT OUTPUT WAS TRUNCATED\")\n",
    "\n",
    "        final_answers.append(correctness)\n",
    "        \n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return final_answers\n",
    "\n",
    "# Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bb4c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "7eacd731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered size: 1000\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"common_sense_1_207_307\",\n",
      "  \"type\": \"common_sense\",\n",
      "  \"prompt\": \"Approximately what percentage of the global population is made up of the ethnic group Princess Fragrant was produced to improve relations with?\",\n",
      "  \"expected\": \"17%\",\n",
      "  \"char_count\": 143,\n",
      "  \"exp_word_count\": 1\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 4/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "10%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "match counts: {'17%': 0}\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"math_3_107_707\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?\",\n",
      "  \"expected\": \"There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\\nSo in Mark's garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\\nPurple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\\nThat means in Mark's garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\\nSo in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\\n#### 35\",\n",
      "  \"char_count\": 277,\n",
      "  \"exp_word_count\": 69\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 69/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Yellow flowers: 10  \n",
       "Purple flowers: 10 + (80% of 10) = 18  \n",
       "Yellow + Purple = 28  \n",
       "Green flowers: 25% of 28 = 7  \n",
       "Total flowers: 10 + 18 + 7 = 35"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/33\n",
      "match counts: {'there': 0, 'are': 0, '80/100': 0, '*': 0, '10': 3, '=': 4, '<<80/100*10=8>>8': 0, 'more': 0, 'purple': 2, 'flowers': 0, 'than': 0, 'yellow': 2, 'flowers.': 0, 'so': 0, 'in': 0, \"mark's\": 0, 'garden,': 0, '+': 4, '8': 0, '<<10+8=18>>18': 0, 'and': 0, 'sum': 0, 'up': 0, 'to': 0, '18': 2, '<<10+18=28>>28': 0, 'that': 0, 'means': 0, 'garden': 0, '25/100': 0, '28': 2, '<<25/100*28=7>>7': 0, 'green': 1, 'total': 1, 'mark': 0, 'has': 0, '7': 2, '<<28+7=35>>35': 0, 'plants': 0, 'his': 0, 'garden.': 0, '####': 0, '35': 1}\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: True\n",
      "‚ùå INCORRECT\n",
      "üÜò DISAGREED\n",
      "\n",
      "DISAGREEMENT --> SECOND EVAL\n",
      "--------------------------------\n",
      "self_eval2: True\n",
      "‚úÖ CORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"planning_4_38_938\",\n",
      "  \"type\": \"planning\",\n",
      "  \"prompt\": \"I am playing with a set of objects. Here are the actions I can do\\n\\nPaltry object_0 object_1 object_2.\\nSip object_0 object_1 object_2.\\nClip object_0 object_1 object_2.\\nWretched object_0 object_1 object_2 object_3.\\nMemory object_0 object_1 object_2.\\nTightfisted object_0 object_1 object_2.\\n\\nI have the following restrictions on my actions:\\nTo perform paltry action, the following facts need to be true: hand object_0, cats object_1, texture object_2, vase object_0 object_1, and next object_1 object_2\\nOnce paltry is performed the following facts will be true: next object_0 object_2\\nOnce paltry is performed the following facts will be false: vase object_0 object_1\\nTo perform sip action, the following facts need to be true: hand object_0, cats object_1, texture object_2, next object_0 object_2, and next object_1 object_2\\nOnce sip is performed the following facts will be true: vase object_0 object_1\\nOnce sip is performed the following facts will be false: next object_0 object_2\\nTo perform clip action, the following facts need to be true: hand object_0, sneeze object_1, texture object_2, next object_1 object_2, and next object_0 object_2\\nOnce clip is performed the following facts will be true: vase object_0 object_1\\nOnce clip is performed the following facts will be false: next object_0 object_2\\nTo perform wretched action, the following facts need to be true: sneeze object_0, texture object_1, texture object_2, stupendous object_3, next object_0 object_1, collect object_1 object_3, and collect object_2 object_3\\nOnce wretched is performed the following facts will be true: next object_0 object_2\\nOnce wretched is performed the following facts will be false: next object_0 object_1\\nTo perform memory action, the following facts need to be true: cats object_0, spring object_1, spring object_2, and next object_0 object_1\\nOnce memory is performed the following facts will be true: next object_0 object_2\\nOnce memory is performed the following facts will be false: next object_0 object_1\\nTo perform tightfisted action, the following facts need to be true: hand object_0, sneeze object_1, texture object_2, next object_1 object_2, and vase object_0 object_1\\nOnce tightfisted is performed the following facts will be true: next object_0 object_2\\nOnce tightfisted is performed the following facts will be false: vase object_0 object_1\\n\\n[STATEMENT]\\nAs initial conditions I have that, cats object_0, collect object_7 object_1, collect object_8 object_2, collect object_9 object_3, hand object_10, hand object_11, next object_0 object_8, next object_10 object_8, next object_11 object_9, next object_4 object_7, next object_5 object_8, next object_6 object_9, sneeze object_4, sneeze object_5, sneeze object_6, spring object_7, spring object_8, spring object_9, stupendous object_1, stupendous object_2, stupendous object_3, texture object_7, texture object_8 and texture object_9.\\nMy goal is to have that next object_10 object_9 and next object_11 object_7.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nsip object_10 object_0 object_8\\nmemory object_0 object_8 object_9\\nsip object_11 object_0 object_9\\npaltry object_10 object_0 object_9\\nmemory object_0 object_9 object_7\\npaltry object_11 object_0 object_7\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, cats object_0, collect object_7 object_1, collect object_8 object_2, collect object_9 object_3, hand object_10, hand object_11, next object_0 object_8, next object_10 object_9, next object_11 object_8, next object_4 object_7, next object_5 object_8, next object_6 object_9, sneeze object_4, sneeze object_5, sneeze object_6, spring object_7, spring object_8, spring object_9, stupendous object_1, stupendous object_2, stupendous object_3, texture object_7, texture object_8 and texture object_9.\\nMy goal is to have that next object_10 object_9 and next object_11 object_9.\\n\\nMy plan is as follows:\\n\\n[PLAN]\",\n",
      "  \"expected\": \"(sip o11 o0 o8)\\n(memory o0 o8 o9)\\n(paltry o11 o0 o9)\\n\",\n",
      "  \"char_count\": 3861,\n",
      "  \"exp_word_count\": 12\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 72/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "sip object_10 object_0 object_9  \n",
       "memory object_0 object_9 object_8  \n",
       "sip object_11 object_0 object_9  \n",
       "paltry object_11 object_0 object_9  \n",
       "memory object_0 object_9 object_7  \n",
       "paltry object_10 object_0 object_7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/24\n",
      "match counts: {'(sip': 0, 'o11': 0, 'o0': 0, 'o8)': 0, '(memory': 0, 'o8': 0, 'o9)': 0, '(paltry': 0}\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"planning_4_47_947\",\n",
      "  \"type\": \"planning\",\n",
      "  \"prompt\": \"I am playing with a set of blocks where I need to arrange the blocks into stacks. Here are the actions I can do\\n\\nPick up a block\\nUnstack a block from on top of another block\\nPut down a block\\nStack a block on top of another block\\n\\nI have the following restrictions on my actions:\\nI can only pick up or unstack one block at a time.\\nI can only pick up or unstack a block if my hand is empty.\\nI can only pick up a block if the block is on the table and the block is clear. A block is clear if the block has no other blocks on top of it and if the block is not picked up.\\nI can only unstack a block from on top of another block if the block I am unstacking was really on top of the other block.\\nI can only unstack a block from on top of another block if the block I am unstacking is clear.\\nOnce I pick up or unstack a block, I am holding the block.\\nI can only put down a block that I am holding.\\nI can only stack a block on top of another block if I am holding the block being stacked.\\nI can only stack a block on top of another block if the block onto which I am stacking the block is clear.\\nOnce I put down or stack a block, my hand becomes empty.\\nOnce you stack a block on top of a second block, the second block is no longer clear.\\n\\n[STATEMENT]\\nAs initial conditions I have that, the red block is clear, the yellow block is clear, the hand is empty, the red block is on top of the white block, the blue block is on top of the orange block, the white block is on top of the blue block, the orange block is on the table and the yellow block is on the table.\\nMy goal is to have that the blue block is on top of the yellow block and the white block is on top of the red block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\\nunstack the red block from on top of the white block\\nput down the red block\\nunstack the white block from on top of the blue block\\nstack the white block on top of the red block\\nunstack the blue block from on top of the orange block\\nstack the blue block on top of the yellow block\\n[PLAN END]\\n\\n[STATEMENT]\\nAs initial conditions I have that, the yellow block is clear, the white block is clear, the hand is empty, the red block is on top of the blue block, the blue block is on top of the orange block, the yellow block is on top of the red block, the orange block is on the table and the white block is on the table.\\nMy goal is to have that the red block is on top of the orange block, the yellow block is on top of the red block and the white block is on top of the blue block.\\n\\nMy plan is as follows:\\n\\n[PLAN]\",\n",
      "  \"expected\": \"(unstack yellow red)\\n(stack yellow white)\\n(unstack red blue)\\n(stack red yellow)\\n(unstack blue orange)\\n(put-down blue)\\n(unstack red yellow)\\n(stack red orange)\\n(unstack yellow white)\\n(stack yellow red)\\n(pick-up white)\\n(stack white blue)\\n\",\n",
      "  \"char_count\": 2511,\n",
      "  \"exp_word_count\": 34\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 74/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "unstack the yellow block from on top of the red block  \n",
       "put down the yellow block  \n",
       "unstack the red block from on top of the blue block  \n",
       "stack the red block on top of the orange block  \n",
       "stack the yellow block on top of the red block  \n",
       "unstack the white block from the table  \n",
       "stack the white block on top of the blue block"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/64\n",
      "match counts: {'(unstack': 0, 'yellow': 3, 'red)': 0, '(stack': 0, 'white)': 0, 'red': 4, 'blue)': 0, 'yellow)': 0, 'blue': 2, 'orange)': 0, '(put-down': 0, '(pick-up': 0, 'white': 2}\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "TEST_CASE\n",
      "--------------------------------\n",
      "{\n",
      "  \"id\": \"math_3_119_719\",\n",
      "  \"type\": \"math\",\n",
      "  \"prompt\": \"James is a first-year student at a University in Chicago. He has a budget of $1000 per semester. He spends 30% of his money on food, 15% on accommodation, 25% on entertainment, and the rest on coursework materials. How much money does he spend on coursework materials?\",\n",
      "  \"expected\": \"Accommodation is 15% * $1000=$<<15*.01*1000=150>>150\\nFood is          30% * $1000=$<<30*.01*1000=300>>300\\nEntertainment is   25% * $1000=$<<25*.01*1000=250>>250\\nCoursework materials are thus $1000-($150+$300+$250) = $300\\n#### 300\",\n",
      "  \"char_count\": 268,\n",
      "  \"exp_word_count\": 24\n",
      "}\n",
      "\n",
      "MODEL_OUTPUT (TOKENS USED: 5/400)\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "$200"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1\n",
      "match counts: {'accommodation': 0, 'is': 0, '15%': 0, '*': 0, '$1000=$<<15*.01*1000=150>>150': 0, 'food': 0, '30%': 0, '$1000=$<<30*.01*1000=300>>300': 0, 'entertainment': 0, '25%': 0, '$1000=$<<25*.01*1000=250>>250': 0, 'coursework': 0, 'materials': 0, 'are': 0, 'thus': 0, '$1000-($150+$300+$250)': 0, '=': 0, '$300': 0, '####': 0, '300': 0}\n",
      "\n",
      "EVALUATION\n",
      "--------------------------------\n",
      "match check: False\n",
      "self_eval: False\n",
      "‚ùå INCORRECT\n",
      "üÜó AGREED\n",
      "\n",
      " ================================================================\n",
      "[False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "test_prompts = get_tests(n=5) #get_test_type([\"math\"],end=10, upper=300) get_random_tests(n=3, upper=300)\n",
    "results_llm_judge = self_evaluate_tests(test_prompts, verbose=True, model=MODEL, grader_model=MODEL)\n",
    "print(\"\\n\",\"=\"*64)\n",
    "print(results_llm_judge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
